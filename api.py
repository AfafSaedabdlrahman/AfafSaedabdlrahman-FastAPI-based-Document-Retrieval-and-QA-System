from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
from langchain_community.llms import Ollama
from langchain_community.vectorstores import PGVector
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from pdf_processing import preprocess_query, preprocess_and_store_pdf

# Initialize the FastAPI app
app = FastAPI()

# Set up the embedding model using HuggingFace
embedding_function = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L6-v2')

# Configure the PGVector database for storing and retrieving document embeddings
db = PGVector(
    collection_name='documents',  # The name of the collection in the database
    connection_string='postgresql+psycopg2://postgres:test@localhost:5432/vector_db',  # Connection string for PostgreSQL
    embedding_function=embedding_function  # The function to create embeddings
)

# Initialize the language model using Ollama
llm = Ollama(model="smollm",temperature=0)

# Define a template for the question-answering chain prompt
qna_prompt = PromptTemplate(
    input_variables=["context", "question"],  # Input variables that the template will use
    template="Based on the following context, answer the question.\n\nContext: {context}\n\nQuestion: {question}\nAnswer:"
)

# Load the QA chain using the initialized language model and prompt template
qa_chain = load_qa_chain(llm, chain_type="stuff", prompt=qna_prompt)

# Define a Pydantic model to represent an interaction (query and response)
class Interaction(BaseModel):
    query: str  # The user's query
    response: str  # The response generated by the QA system
    timestamp: str  # The timestamp of when the interaction occurred

# In-memory storage to keep track of past interactions
interactions: List[Interaction] = []

# API endpoint to submit a query and receive a response
@app.post("/api/query/")
async def submit_query(query: str):
    # Preprocess the query to ensure it's in a suitable format
    preprocessed_query = preprocess_query(query)

    # Perform a similarity search in the vector database to find relevant documents
    similar_docs = db.similarity_search(preprocessed_query, k=3)

    # Combine the content of the retrieved documents to form the context
    context = " ".join([doc.page_content for doc in similar_docs])
    
    # Use the QA chain to generate an answer based on the context and the preprocessed query
    answer = qa_chain.invoke({
        "input_documents": similar_docs,
        "question": preprocessed_query
    }, return_only_outputs=True)
    
    # Extract the response text from the QA chain output
    response_text = answer.get('output_text', 'No answer generated')
    
    # Record the interaction, including the original query and the generated response
    interaction = Interaction(query=query, response=response_text, timestamp="current_timestamp_here")  # Replace with actual timestamp
    interactions.append(interaction)
    
    # Return the query and its corresponding response as the API output
    return {"query": query, "response": response_text}

# API endpoint to retrieve all past interactions
@app.get("/api/interactions/")
async def get_interactions():
    # Return the list of past interactions
    return interactions
